# -*- coding: utf-8 -*-
"""Predictive Text Generation using GPT Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A4bwb-fI7aslyDR83Oz1G6Rb1lbTx2Qw
"""

!pip install transformers datasets

!pip install pyarrow==14.0.1

!pip install cudf-cu12 ibis-framework --upgrade

"""# New section"""

from google.colab import files
files.upload()  # Select the kaggle.json file from your local machine

!pip install kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d himonsarkar/openwebtext-dataset
!unzip openwebtext-dataset.zip -d openwebtext

!rm -rf /content/*

import os

data_path = 'openwebtext'
if not os.path.exists(data_path):
    os.makedirs(data_path)

all_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.txt')]

max_size_gb = 15  # Limit to 15 GB
current_size = 0
max_size_bytes = max_size_gb * (1024**3)

with open('dataset.txt', 'w', encoding='utf-8') as outfile:
    for fname in all_files:
        with open(fname, 'r', encoding='utf-8') as infile:
            for line in infile:
                line_size = len(line.encode('utf-8'))  # Calculate size of the line
                if current_size + line_size > max_size_bytes:
                    print("Reached 15 GB limit")
                    break
                outfile.write(line + '\n')
                current_size += line_size

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import load_dataset

!split -n l/90 dataset.txt train.txt
!split -n l/10 dataset.txt eval.txt

with open('dataset.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()

split_point = int(0.9 * len(lines))
with open('train.txt', 'w', encoding='utf-8') as f:
    f.writelines(lines[:split_point])
with open('eval.txt', 'w', encoding='utf-8') as f:
    f.writelines(lines[split_point:])

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def load_dataset(train_path, eval_path, tokenizer):
    train_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=train_path,
        block_size=128)

    eval_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=eval_path,
        block_size=128)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False)

    return train_dataset, eval_dataset, data_collator

train_dataset, eval_dataset, data_collator = load_dataset('train.txt', 'eval.txt', tokenizer)

training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=1,             # You can increase this for better results
    per_device_train_batch_size=4,  # Adjust based on your GPU capacity
    per_device_eval_batch_size=4,
    evaluation_strategy='steps',
    eval_steps=500,                 # Evaluate every 500 steps
    save_steps=500,                 # Save checkpoint every 500 steps
    warmup_steps=500,
    prediction_loss_only=True,
    logging_steps=100,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.save_model('./gpt2-finetuned-openwebtext')
tokenizer.save_pretrained('./gpt2-finetuned-openwebtext')

from transformers import pipeline

generator = pipeline('text-generation', model='./gpt2-finetuned-openwebtext', tokenizer=tokenizer)

prompt = "ali friend"
outputs = generator(prompt, max_length=50, num_return_sequences=1, temperature=0.7)

print(outputs[0]['generated_text'])

from google.colab import drive
drive.mount('/content/drive')

from transformers import pipeline

generator = pipeline('text-generation', model='gpt2', device=0)  # device=0 refers to the first GPU



from google.colab import drive
drive.mount('/content/drive')

train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path='train.txt',
    block_size=512)  # Adjust as needed

model.gradient_checkpointing_enable()

outputs = generator(
    prompt,
    max_length=100,
    num_return_sequences=1,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = GPT2LMHeadModel.from_pretrained('gpt2-medium')